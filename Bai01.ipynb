{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn.impute   import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics  import accuracy_score, auc, roc_curve, precision_recall_curve, roc_auc_score, precision_score, recall_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost  import XGBClassifier\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testing_set_x, testing_set_y):\n",
    "    predictions = model.predict_proba(testing_set_x)\n",
    "    \n",
    "    accuracy  = accuracy_score(testing_set_y, predictions[:,1] >= 0.5)\n",
    "    roc_auc   = roc_auc_score(testing_set_y, predictions[:,1])\n",
    "    precision = precision_score(testing_set_y, predictions[:,1] >= 0.5)\n",
    "    recall    = recall_score(testing_set_y, predictions[:,1] >= 0.5)\n",
    "    pr_auc    = average_precision_score(testing_set_y, predictions[:,1])\n",
    "    \n",
    "    result = pd.DataFrame([[accuracy, precision, recall, roc_auc, pr_auc]], columns=['Accuracy', 'Precision', 'Recall', 'ROC_auc','PR_auc'])\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(df, model_class, n = 100, **kwargs):\n",
    "    results = pd.DataFrame(columns=['Accuracy', 'Precision', 'Recall', 'ROC_auc','PR_auc'])\n",
    "    for i in range(n):\n",
    "        # Compose dataset\n",
    "        train_x, test_x = train_test_split(df.drop('PATIENT_VISIT_IDENTIFIER', axis=1),\n",
    "                               test_size = 0.3,\n",
    "                               stratify  = df['ICU'],\n",
    "                               random_state = i\n",
    "                                )\n",
    "        \n",
    "        train_y = train_x.pop('ICU')\n",
    "        test_y  = test_x.pop('ICU')\n",
    "        \n",
    "        # Train Model\n",
    "        model = model_class(**kwargs)\n",
    "        model.fit(train_x, train_y)\n",
    "         \n",
    "        # Evaluate results\n",
    "        current_result = evaluate(model, test_x, test_y)\n",
    "        results = results.append(current_result)\n",
    "        \n",
    "    return(results.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(df, plot = True, extras = False, color='dodgerblue'):\n",
    "    print('|||||||||||||||||||||||||||||||||||||||||||||||||||||||')\n",
    "    print('[ Experiment Results ]')\n",
    "    print('Accuracy:   {}'.format(df.Accuracy.mean()))\n",
    "    print('Precision:  {}'.format(df.Precision.mean()))\n",
    "    print('Recall:     {}'.format(df.Recall.mean()))\n",
    "    print('ROC Auc:    {}'.format(df.ROC_auc.mean()))\n",
    "    print('PR Auc:     {}'.format(df.PR_auc.mean()))\n",
    "    print('|||||||||||||||||||||||||||||||||||||||||||||||||||||||')\n",
    "    \n",
    "    if plot:\n",
    "        fig = px.box(df.melt(var_name='metric'),\n",
    "                       y = 'metric',\n",
    "                       x = 'value',\n",
    "                       title = 'Distribution of Metric Values Across 100 Runs',\n",
    "                       color_discrete_sequence=[color]\n",
    "                      )\n",
    "\n",
    "        fig.update_xaxes(title='Metric')\n",
    "        fig.update_yaxes(title='Value')\n",
    "\n",
    "        fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 00)',\n",
    "                           'paper_bgcolor': 'rgba(240, 240, 240, 100)'})\n",
    "        fig.show()\n",
    "        \n",
    "        \n",
    "    if extras:\n",
    "        print('Also, the maximum results were:')\n",
    "        print('    Accuracy:   {}'.format(df.Accuracy.max()))\n",
    "        print('    Precision:  {}'.format(df.Precision.max()))\n",
    "        print('    Recall:     {}'.format(df.Recall.max()))\n",
    "        print('    ROC Auc:    {}'.format(df.ROC_auc.max()))\n",
    "        print('    PR Auc:     {}'.format(df.PR_auc.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "raw_data = pd.read_excel('Kaggle_Sirio_Libanes_ICU_Prediction.xlsx')\n",
    "raw_data.sample(5)\n",
    "\n",
    "# Data Preparation\n",
    "raw_data['AGE_PERCENTIL'] = raw_data['AGE_PERCENTIL'].str.replace('Above ','').str.extract(r'(.+?)th')\n",
    "raw_data['WINDOW'] = raw_data['WINDOW'].str.replace('ABOVE_12','12-more').str.extract(r'(.+?)-')\n",
    "\n",
    "# Missingness as features\n",
    "raw_data['row_missingness'] = raw_data.isnull().sum(axis=1)\n",
    "\n",
    "# Mean imputation\n",
    "mean_impute  = SimpleImputer(strategy='mean')\n",
    "imputed_data = mean_impute.fit_transform(raw_data)\n",
    "imputed_data = pd.DataFrame(imputed_data, columns = raw_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['ICU'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_optimal = {\n",
    "              'n_estimators':2100,\n",
    "              'max_depth':27,\n",
    "              'max_features':0.15,\n",
    "              'max_samples':0.5363991145732665,\n",
    "              'min_samples_split':2,\n",
    "              'min_samples_leaf':4,\n",
    "              'n_jobs':-1,\n",
    "              'random_state':451,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_experiment = run_experiment(imputed_data, model_class = RandomForestClassifier, **rf_optimal)\n",
    "print_results(rf_experiment, color = '#3F3F3F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
